{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kMgvn98l6Yop",
        "outputId": "f709f52f-f1e5-4b28-bebc-1041319b97cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install google-generativeai pandas numpy joblib scikit-learn xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import google.generativeai as genai\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set your API key\n",
        "GOOGLE_API_KEY = \"AIzaSyC8BD4pCVWHt1mD5fXzZtVzr4n6wbc3CAM\"  # Replace with your actual API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "#check whether api is working by sending random info\n",
        "# Test if the Gemini API is working\n",
        "try:\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "    response = model.generate_content(\"Hello, Gemini! Can you hear me?\")\n",
        "    print(\"Gemini API test successful. Response:\", response.text)\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Gemini API: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "rsYGKGbr7tQi",
        "outputId": "e260cd9a-bd6f-4dc4-f318-a560f6a3d2f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API test successful. Response: Yes, I can hear you! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    \"\"\"Load the specified model from the models directory.\"\"\"\n",
        "    model_path = None\n",
        "\n",
        "    # Check if model exists as .joblib file\n",
        "    joblib_path = f\"models/{model_name}.joblib\"\n",
        "    if os.path.exists(joblib_path):\n",
        "        model_path = joblib_path\n",
        "        model = joblib.load(model_path)\n",
        "\n",
        "    # Check if model exists as .pkl file\n",
        "    if model_path is None:\n",
        "        pkl_path = f\"models/{model_name}.pkl\"\n",
        "        if os.path.exists(pkl_path):\n",
        "            model_path = pkl_path\n",
        "            with open(pkl_path, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "\n",
        "    # If model doesn't exist, raise an error\n",
        "    if model_path is None:\n",
        "        raise FileNotFoundError(f\"Model file for {model_name} not found in models directory\")\n",
        "\n",
        "    # Return the model and its feature names\n",
        "    return model, MODEL_INFO[model_name]['feature_names']"
      ],
      "metadata": {
        "id": "Z6fTitoL90Gb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model information (features required by each model)\n",
        "MODEL_INFO = {\n",
        "    'apache_model': {\n",
        "        'feature_names': ['Message', 'hour', 'minute'],\n",
        "        'is_pipeline': True\n",
        "    },\n",
        "    'linux_log': {\n",
        "        'feature_names': ['combined_text', 'hour', 'minute'],\n",
        "        'is_pipeline': True\n",
        "    },\n",
        "    'server_log': {\n",
        "        'feature_names': ['Duration', 'Packets', 'Flows', 'Src Pt', 'Dst Pt', 'Bytes_num', 'hour', 'Proto'],\n",
        "        'is_pipeline': True\n",
        "    },\n",
        "    'ssh_login': {\n",
        "        'feature_names': ['hour', 'minute', 'day_of_week', 'user_encoded', 'password_encoded'],\n",
        "        'is_pipeline': False\n",
        "    },\n",
        "    'weblog': {\n",
        "        'feature_names': ['Request', 'Method', 'hour', 'minute'],\n",
        "        'is_pipeline': True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print available models and their required features\n",
        "for model_name, info in MODEL_INFO.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Features: {info['feature_names']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfK-0tWM8H1-",
        "outputId": "8b01db60-cf8b-4dc9-b42b-dddaeeeae21a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "apache_model:\n",
            "  Features: ['Message', 'hour', 'minute']\n",
            "\n",
            "linux_log:\n",
            "  Features: ['combined_text', 'hour', 'minute']\n",
            "\n",
            "server_log:\n",
            "  Features: ['Duration', 'Packets', 'Flows', 'Src Pt', 'Dst Pt', 'Bytes_num', 'hour', 'Proto']\n",
            "\n",
            "ssh_login:\n",
            "  Features: ['hour', 'minute', 'day_of_week', 'user_encoded', 'password_encoded']\n",
            "\n",
            "weblog:\n",
            "  Features: ['Request', 'Method', 'hour', 'minute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_log_with_gemini(model_name):\n",
        "    \"\"\"Use Gemini API to generate a log message for the specified model type.\"\"\"\n",
        "    # Prepare prompts based on the model type with specific format instructions\n",
        "    prompts = {\n",
        "        'apache_model': \"\"\"Generate only ONE realistic Apache web server log entry in this format:\n",
        "IP_ADDRESS - - [DAY/MONTH/YEAR:HOUR:MINUTE:SECOND +TIMEZONE] \"METHOD /PATH HTTP/1.1\" STATUS_CODE BYTES_SENT \"REFERER\" \"USER_AGENT\"\n",
        "\n",
        "Example: 192.168.1.1 - - [25/Mar/2023:10:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 1024 \"http://example.com\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "\n",
        "The log should include:\n",
        "- A timestamp with hour and minute\n",
        "- HTTP request details\n",
        "\n",
        "IMPORTANT: Generate only a SINGLE log line, not multiple entries.\"\"\",\n",
        "\n",
        "        'linux_log': \"\"\"Generate only ONE realistic Linux system log entry in this format:\n",
        "Month Day HH:MM:SS HOSTNAME SERVICE[PID]: MESSAGE\n",
        "\n",
        "Example: Mar 25 10:15:23 server kernel[1234]: CPU temperature above threshold, throttling CPU\n",
        "\n",
        "The log should include:\n",
        "- A timestamp with hour and minute\n",
        "- A service name and message\n",
        "\n",
        "IMPORTANT: Generate only a SINGLE log line, not multiple entries.\"\"\",\n",
        "\n",
        "        'server_log': \"\"\"Generate only ONE realistic server performance log with these specific metrics:\n",
        "Duration: NUMBER Packets: NUMBER Flows: NUMBER Src Port: NUMBER Dst Port: NUMBER Bytes: NUMBER Protocol: PROTOCOL_NAME\n",
        "\n",
        "Example: Client 192.168.1.10 connected to server - Duration: 145 Packets: 23 Flows: 3 Src Port: 34567 Dst Port: 80 Bytes: 45678 Protocol: TCP\n",
        "\n",
        "The log must include exact numeric values for:\n",
        "- Duration, Packets, Flows, Src Port (Src Pt), Dst Port (Dst Pt), Bytes\n",
        "- Protocol should be one of: TCP, UDP, or ICMP\n",
        "- Include a timestamp with hour\n",
        "\n",
        "IMPORTANT: Generate only a SINGLE log line, not multiple entries.\"\"\",\n",
        "\n",
        "        'ssh_login': \"\"\"Generate only ONE realistic SSH login attempt log in this format:\n",
        "Month Day HH:MM:SS HOSTNAME sshd[PID]: MESSAGE\n",
        "\n",
        "Example: Mar 25 10:15:23 server sshd[1234]: Failed password for invalid user admin from 192.168.1.100 port 22 ssh2\n",
        "\n",
        "The log should include:\n",
        "- A timestamp with hour and minute\n",
        "- Day of week information\n",
        "- User information\n",
        "- Password status (valid/invalid)\n",
        "\n",
        "IMPORTANT: Generate only a SINGLE log line, not multiple entries.\"\"\",\n",
        "\n",
        "        'weblog': \"\"\"Generate only ONE realistic web application log entry in this format:\n",
        "[TIMESTAMP] \"METHOD /PATH HTTP/1.1\" STATUS_CODE RESPONSE_TIME \"REFERER\" IP_ADDRESS USER_ID\n",
        "\n",
        "Example: [2023-03-25 10:15:23] \"GET /login HTTP/1.1\" 200 5.2ms \"http://example.com\" 192.168.1.100 user123\n",
        "\n",
        "The log must include:\n",
        "- HTTP method (GET, POST, PUT, DELETE)\n",
        "- URL path\n",
        "- A timestamp with hour and minute\n",
        "\n",
        "IMPORTANT: Generate only a SINGLE log line, not multiple entries.\"\"\"\n",
        "    }\n",
        "\n",
        "    # Get the appropriate prompt\n",
        "    prompt = prompts.get(model_name, f\"Generate a single realistic {model_name} log entry.\")\n",
        "\n",
        "    # Additional instructions for better output\n",
        "    prompt += \"\\n\\nIMPORTANT: Return ONLY ONE raw log text without any additional explanation, markdown formatting, or quotes. Do not generate multiple log entries.\"\n",
        "\n",
        "    # Call the Gemini API\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Extract just the log text\n",
        "    log_text = response.text.strip()\n",
        "    if log_text.startswith('```') and log_text.endswith('```'):\n",
        "        log_text = log_text.strip('`').strip()\n",
        "\n",
        "    # Get the first line if multiple lines are returned\n",
        "    log_text = log_text.split('\\n')[0]\n",
        "\n",
        "    return log_text"
      ],
      "metadata": {
        "id": "kSoPcvrD8Y9r"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_log_to_features(log_text, model_name):\n",
        "    \"\"\"Parse a log message into the features needed by the specified model.\"\"\"\n",
        "    # Use Gemini to extract structured data from the log text\n",
        "    features = MODEL_INFO[model_name]['feature_names']\n",
        "\n",
        "    # If it's a server log, try to extract features directly with regex first\n",
        "    if model_name == 'server_log':\n",
        "        import re\n",
        "        try:\n",
        "            # Extract hour\n",
        "            hour = datetime.now().hour  # Default\n",
        "            time_match = re.search(r'(\\d{1,2}):\\d{1,2}:\\d{1,2}', log_text)\n",
        "            if time_match:\n",
        "                hour = int(time_match.group(1))\n",
        "\n",
        "            # Extract performance metrics\n",
        "            duration = re.search(r'Duration:\\s*(\\d+)', log_text)\n",
        "            packets = re.search(r'Packets:\\s*(\\d+)', log_text)\n",
        "            flows = re.search(r'Flows:\\s*(\\d+)', log_text)\n",
        "            src_pt = re.search(r'Src\\s*P[or]*t:\\s*(\\d+)', log_text)\n",
        "            dst_pt = re.search(r'Dst\\s*P[or]*t:\\s*(\\d+)', log_text)\n",
        "            bytes_num = re.search(r'Bytes:\\s*(\\d+)', log_text)\n",
        "            proto = re.search(r'Protocol:\\s*(\\w+)', log_text)\n",
        "\n",
        "            if all([duration, packets, flows, src_pt, dst_pt, bytes_num, proto]):\n",
        "                return {\n",
        "                    'Duration': int(duration.group(1)),\n",
        "                    'Packets': int(packets.group(1)),\n",
        "                    'Flows': int(flows.group(1)),\n",
        "                    'Src Pt': int(src_pt.group(1)),\n",
        "                    'Dst Pt': int(dst_pt.group(1)),\n",
        "                    'Bytes_num': int(bytes_num.group(1)),\n",
        "                    'hour': hour,\n",
        "                    'Proto': proto.group(1)\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"Error in direct extraction: {e} - falling back to Gemini\")\n",
        "\n",
        "    # If direct extraction failed or for other model types, use Gemini\n",
        "    feature_explanations = {\n",
        "        'apache_model': \"\"\"\n",
        "- 'Message': The HTTP request part (e.g., \"GET /index.html HTTP/1.1\")\n",
        "- 'hour': Hour extracted from timestamp (0-23)\n",
        "- 'minute': Minute extracted from timestamp (0-59)\n",
        "        \"\"\",\n",
        "\n",
        "        'linux_log': \"\"\"\n",
        "- 'combined_text': Combine the service name and message text\n",
        "- 'hour': Hour extracted from timestamp (0-23)\n",
        "- 'minute': Minute extracted from timestamp (0-59)\n",
        "        \"\"\",\n",
        "\n",
        "        'server_log': \"\"\"\n",
        "- 'Duration': Numeric value after \"Duration:\" (must be an integer)\n",
        "- 'Packets': Numeric value after \"Packets:\" (must be an integer)\n",
        "- 'Flows': Numeric value after \"Flows:\" (must be an integer)\n",
        "- 'Src Pt': Numeric value of source port (must be an integer)\n",
        "- 'Dst Pt': Numeric value of destination port (must be an integer)\n",
        "- 'Bytes_num': Numeric value of bytes (must be an integer)\n",
        "- 'hour': Hour extracted from timestamp (must be an integer 0-23)\n",
        "- 'Proto': Protocol name (TCP, UDP, or ICMP) (must be a string)\n",
        "        \"\"\",\n",
        "\n",
        "        'ssh_login': \"\"\"\n",
        "- 'hour': Hour extracted from timestamp (0-23) (must be an integer)\n",
        "- 'minute': Minute extracted from timestamp (0-59) (must be an integer)\n",
        "- 'day_of_week': Day of week as number (0=Monday, 1=Tuesday, ..., 6=Sunday) (must be an integer)\n",
        "- 'user_encoded': 0 for 'root', 1 for 'admin', 2 for any other user (must be an integer)\n",
        "- 'password_encoded': 0 for valid password, 1 for invalid password, 2 for unknown (must be an integer)\n",
        "        \"\"\",\n",
        "\n",
        "        'weblog': \"\"\"\n",
        "- 'Request': The URL path portion (e.g., \"/login\", \"/profile\", etc.) (must be a string)\n",
        "- 'Method': The HTTP method (GET, POST, PUT, DELETE) (must be a string)\n",
        "- 'hour': Hour extracted from timestamp (0-23) (must be an integer)\n",
        "- 'minute': Minute extracted from timestamp (0-59) (must be an integer)\n",
        "        \"\"\"\n",
        "    }\n",
        "\n",
        "    explanation = feature_explanations.get(model_name, \"\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Parse the following log message and extract exactly these features: {features}.\n",
        "\n",
        "    Log: {log_text}\n",
        "\n",
        "    Feature details:{explanation}\n",
        "\n",
        "    Return ONLY a valid JSON object with the feature names as keys matching these exact requirements:\n",
        "    1. Include ONLY the specified feature names as keys\n",
        "    2. For numeric features, use integer values (not strings)\n",
        "    3. For text features, use strings\n",
        "    4. If you can't extract a feature, use reasonable defaults\n",
        "\n",
        "    Example output for server_log:\n",
        "    {{\"Duration\": 145, \"Packets\": 23, \"Flows\": 3, \"Src Pt\": 34567, \"Dst Pt\": 80, \"Bytes_num\": 45678, \"hour\": 10, \"Proto\": \"TCP\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the Gemini API\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Parse the JSON response\n",
        "    try:\n",
        "        # Debug information\n",
        "        print(f\"Gemini response for feature extraction: {response.text.strip()}\")\n",
        "\n",
        "        # Clean up the response to ensure it's valid JSON\n",
        "        clean_text = response.text.strip()\n",
        "        # Remove any markdown backticks if present\n",
        "        if clean_text.startswith('```') and clean_text.endswith('```'):\n",
        "            clean_text = clean_text.strip('`').strip()\n",
        "        # Remove json prefix if present\n",
        "        if clean_text.startswith('json') or clean_text.startswith('JSON'):\n",
        "            clean_text = clean_text[4:].strip()\n",
        "\n",
        "        features_dict = json.loads(clean_text)\n",
        "\n",
        "        # Verify all required features are present\n",
        "        for feature in features:\n",
        "            if feature not in features_dict:\n",
        "                # Add default values for missing features\n",
        "                if feature in ['hour', 'minute', 'day_of_week', 'user_encoded', 'password_encoded']:\n",
        "                    features_dict[feature] = 0\n",
        "                elif feature in ['Duration', 'Packets', 'Flows', 'Src Pt', 'Dst Pt', 'Bytes_num']:\n",
        "                    features_dict[feature] = 100  # Default value\n",
        "                elif feature == 'Proto':\n",
        "                    features_dict[feature] = 'TCP'\n",
        "                else:\n",
        "                    features_dict[feature] = ''\n",
        "\n",
        "        # Ensure all numeric values are actually integers\n",
        "        for feature in ['hour', 'minute', 'day_of_week', 'user_encoded', 'password_encoded',\n",
        "                       'Duration', 'Packets', 'Flows', 'Src Pt', 'Dst Pt', 'Bytes_num']:\n",
        "            if feature in features_dict:\n",
        "                try:\n",
        "                    features_dict[feature] = int(features_dict[feature])\n",
        "                except (ValueError, TypeError):\n",
        "                    # If conversion fails, set default\n",
        "                    features_dict[feature] = 0 if feature in ['hour', 'minute', 'day_of_week', 'user_encoded', 'password_encoded'] else 100\n",
        "\n",
        "        return features_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in JSON parsing: {e}\")\n",
        "        # Create a basic dictionary with default values\n",
        "        now = datetime.now()\n",
        "        defaults = {\n",
        "            'Message': log_text[:50],\n",
        "            'combined_text': log_text[:50],\n",
        "            'Request': '/index.html',\n",
        "            'Method': 'GET',\n",
        "            'hour': now.hour,\n",
        "            'minute': now.minute,\n",
        "            'day_of_week': now.weekday(),\n",
        "            'user_encoded': 2,  # Unknown\n",
        "            'password_encoded': 2,  # Unknown\n",
        "            'Duration': 100,\n",
        "            'Packets': 10,\n",
        "            'Flows': 1,\n",
        "            'Src Pt': 12345,\n",
        "            'Dst Pt': 80,\n",
        "            'Bytes_num': 1024,\n",
        "            'Proto': 'TCP'\n",
        "        }\n",
        "        # Filter to only include the features needed by this model\n",
        "        return {k: defaults.get(k, 0) for k in features}"
      ],
      "metadata": {
        "id": "GDb-jhnA8c_e"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_random_log():\n",
        "    \"\"\"Select a random model, generate a log, and classify it.\"\"\"\n",
        "    # 1. Randomly select a model\n",
        "    model_names = list(MODEL_INFO.keys())\n",
        "    selected_model = random.choice(model_names)\n",
        "    print(f\"\\n===== Selected Model: {selected_model} =====\")\n",
        "\n",
        "    # 2. Generate a log message using Gemini\n",
        "    print(\"\\nGenerating log message...\")\n",
        "    log_text = generate_log_with_gemini(selected_model)\n",
        "    print(f\"\\nGenerated log:\\n{log_text}\")\n",
        "\n",
        "    # 3. Parse the log to extract features\n",
        "    print(\"\\nExtracting features...\")\n",
        "    features = parse_log_to_features(log_text, selected_model)\n",
        "    print(f\"Extracted features: {features}\")\n",
        "\n",
        "    # 4. Load the model\n",
        "    print(\"\\nLoading model...\")\n",
        "    model, feature_names = load_model(selected_model)\n",
        "\n",
        "    # 5. Convert features to DataFrame\n",
        "    print(\"\\nPreparing features for classification...\")\n",
        "    features_df = features_to_dataframe(features, feature_names)\n",
        "    print(\"Features DataFrame:\")\n",
        "    print(features_df)\n",
        "\n",
        "    # 6. Classify the log\n",
        "    print(\"\\nClassifying log...\")\n",
        "    try:\n",
        "        # Make prediction\n",
        "        prediction = model.predict(features_df)[0]\n",
        "\n",
        "        # Get probability if available\n",
        "        probabilities = {}\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            try:\n",
        "                probs = model.predict_proba(features_df)[0]\n",
        "                classes = model.classes_\n",
        "                probabilities = {str(classes[i]): float(probs[i]) for i in range(len(classes))}\n",
        "            except Exception as e:\n",
        "                print(f\"Could not get probabilities: {e}\")\n",
        "\n",
        "        # 7. Display the result\n",
        "        print(\"\\n===== Classification Result =====\")\n",
        "        print(f\"Classification: {prediction}\")\n",
        "        print(f\"Interpretation: {'Suspicious/Issue detected' if str(prediction) == '1' else 'Normal log'}\")\n",
        "        if probabilities:\n",
        "            print(\"Probabilities:\")\n",
        "            for cls, prob in probabilities.items():\n",
        "                print(f\"  - Class {cls}: {prob:.4f}\")\n",
        "\n",
        "        # Store the classification category\n",
        "        category = \"suspicious\" if str(prediction) == \"1\" else \"normal\"\n",
        "\n",
        "        return {\n",
        "            \"model\": selected_model,\n",
        "            \"log\": log_text,\n",
        "            \"features\": features,\n",
        "            \"prediction\": str(prediction),\n",
        "            \"category\": category,\n",
        "            \"probabilities\": probabilities\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during classification: {e}\")\n",
        "        return {\n",
        "            \"model\": selected_model,\n",
        "            \"log\": log_text,\n",
        "            \"features\": features,\n",
        "            \"error\": str(e)\n",
        "        }"
      ],
      "metadata": {
        "id": "KFqYQLhL8fFo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and classify a random log\n",
        "result = process_random_log()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "collapsed": true,
        "id": "N8bFVAqc8hTd",
        "outputId": "c4212a85-109c-49fe-fd5d-5a5f21c8d148"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Selected Model: linux_log =====\n",
            "\n",
            "Generating log message...\n",
            "\n",
            "Generated log:\n",
            "Oct 26 14:32:17 fileserver smbd[4567]: User 'johndoe' authentication failed for service 'data'\n",
            "\n",
            "Extracting features...\n",
            "Gemini response for feature extraction: ```json\n",
            "{\n",
            "    \"combined_text\": \"data authentication failed for service 'data'\",\n",
            "    \"hour\": 14,\n",
            "    \"minute\": 32\n",
            "}\n",
            "```\n",
            "Extracted features: {'combined_text': \"data authentication failed for service 'data'\", 'hour': 14, 'minute': 32}\n",
            "\n",
            "Loading model...\n",
            "\n",
            "Preparing features for classification...\n",
            "Features DataFrame:\n",
            "                                   combined_text  hour  minute\n",
            "0  data authentication failed for service 'data'    14      32\n",
            "\n",
            "Classifying log...\n",
            "\n",
            "===== Classification Result =====\n",
            "Classification: 1\n",
            "Interpretation: Suspicious/Issue detected\n",
            "Probabilities:\n",
            "  - Class 0: 0.0087\n",
            "  - Class 1: 0.9913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NnFJXXD8jRR"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}